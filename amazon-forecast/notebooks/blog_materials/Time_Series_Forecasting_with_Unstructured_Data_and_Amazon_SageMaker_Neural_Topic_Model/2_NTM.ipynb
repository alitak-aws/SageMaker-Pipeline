{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Topic Popularity Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the second notebook in a series that goes through an example for building an end to end machine learning model for forecasting news topics.  The question we are asking is: \n",
    "\n",
    "### Given historical trends on the popularity of a particular topic based on News articles and sentiment, how popular will this topic be in the future?\n",
    "\n",
    "The example uses a sample dataset which has been obtained from UCI data repository. The raw dataset can be found here: https://archive.ics.uci.edu/ml/machine-learning-databases/00432/Data/News_Final.csv.\n",
    "Details of the data preparation process can be found here: https://arxiv.org/pdf/1801.07055.pdf <br/>\n",
    "\n",
    "Please first run 1_preprocess.ipynb which runs a series of preprocessing steps on the raw data. This notebook will show you how to build a neural topic model to extract topic vectors from the processed dataset.\n",
    "\n",
    "In a subsequent notebook (3_Forecast.ipynb) we will use Amazon Forecast's DeepAR+ Algorithm for time series forecasting to predict the topic's future popularity.\n",
    "\n",
    "Note: This notebook works in any python 3 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import boto3\n",
    "import sagemaker\n",
    "import nltk\n",
    "# importing forecast notebook utility from notebooks/ directory\n",
    "sys.path.insert( 0, os.path.abspath(\"../../common\"))\n",
    "import util\n",
    "\n",
    "\n",
    "session = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget_bucket = util.create_text_widget( \"bucket_name\", \"input your S3 bucket name\" )\n",
    "PREFIX = 'ntm-deepar'\n",
    "NUM_TOPICS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = text_widget_bucket.value\n",
    "assert BUCKET, \"BUCKET not set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/NewsRatingsdataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Shape of the dataframe is {}\".format(df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PublishDate'] = pd.to_datetime(df['PublishDate'], infer_datetime_format=True)\n",
    "START_DATE = datetime(2015, 11, 1)\n",
    "END_DATE = datetime(2016, 7, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Time Series for a given Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(set(df.Topic))\n",
    "print(\"Available Topics = {}\".format(topics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data is missing from the early years, so let's just look at data from 10-22-2015 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic = 0 # Change this to any of [0, 1]\n",
    "subdf = df[(df['Topic']==topic)&(df['PublishDate']>START_DATE)]\n",
    "subdf = subdf.reset_index().set_index('PublishDate')\n",
    "subdf.index = pd.to_datetime(subdf.index)\n",
    "subdf.head()\n",
    "subdf[[\"LinkedIn\", 'GooglePlus', 'Facebook']].resample(\"1D\").mean().dropna().plot(figsize=(15, 4))\n",
    "subdf[[\"SentimentTitle\", 'SentimentHeadline']].resample(\"1D\").mean().dropna().plot(figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has some hourly information, there are multiple datapoints within a given day. However there isn't enough data in a 24 hour period to create an hourly forecast. Instead we will aggregate the data on a daily basis and use the day information to create a daily forecast.\n",
    "\n",
    "This is pretty reasonable -- here we are forecasting the topic popularity a couple weeks out into the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 1 # Change this to any of [0, 1, 2, 3]\n",
    "subdf = df[(df['Topic']==topic)&(df['PublishDate']>START_DATE)]\n",
    "subdf = subdf.reset_index().set_index('PublishDate')\n",
    "subdf.index = pd.to_datetime(subdf.index)\n",
    "subdf.head()\n",
    "subdf[[\"LinkedIn\", 'GooglePlus', 'Facebook']].resample(\"1D\").mean().dropna().plot(figsize=(15, 4))\n",
    "subdf[[\"SentimentTitle\", 'SentimentHeadline']].resample(\"1D\").mean().dropna().plot(figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only data after the START_DATE since there is very little data before 2015-10-13\n",
    "df = df[(df['PublishDate']>START_DATE)].reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the headlines are 'nan's. Let's do a Regex Match to find those and replace those headlines with empty strings. <br/>\n",
    "<br/>\n",
    "Also note that a number of the ratings are negative, which may denote missing values. Since Negative ratings are not physically meaningful, we convert these to 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Headline'] = df.Headline.replace(np.NaN, '')\n",
    "df = df.replace({'Facebook': -1, 'GooglePlus': -1, 'LinkedIn':-1}, 0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is a difference in scale for the popularity on Facebook versus Linkedin versus GooglePlus. For this example, we will focus on forecasting popularity for Facebook only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the Neural Topic model built-in algorithm within SageMaker for extracting topics from the news headlines. To do so, we need to do some preliminary cleaning and preprocessing of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing: Topic Modeling using NTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lower case\n",
    "df['Headline'] = df['Headline'].str.lower()\n",
    "df['Title'] = df['Title'].str.lower()\n",
    "\n",
    "#remove punctuation marks\n",
    "punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "df['Headline']  = df['Headline'] .apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "df['Title'] = df['Title'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "\n",
    "# remove numbers\n",
    "df['Headline'] = df['Headline'] .str.replace(\"[0-9]\", \" \")\n",
    "df['Title'] = df['Title'].str.replace(\"[0-9]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Headline Column contains similar information as the Titles, but since the Headlines are longer, we drop the titles and just retain the actual headlines. But we will store the titles separately to use a validation set for our Neural Topic Model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_column = df.Title\n",
    "df = df.drop(columns = ['Title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write some code to convert the Titles and headlines into tokens that are suitable for a Neural Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 10000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectors = vectorizer.fit_transform(df.Headline)\n",
    "topic_vectors = vectorizer.fit_transform(title_column)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "\n",
    "# Retain the index of the vectors\n",
    "idx = np.arange(vectors.shape[0])\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type cast the vectors into a sparse array\n",
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "topic_vectors = sparse.csr_matrix(topic_vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Topic Model\n",
    "\n",
    "To extract \"text vectors\", we take the Topics and convert each topic into a 20 (NUM_TOPICS) dimensional \"Topic\" vector. This can be viewed as an effective lower dimensional embedding of all the text in the corpus into some predefined topics. Each Topic will have a representation as a vector, and related topics will have a related vector representation. Assuming that there is some correlation between topics from one day to the next., i.e, the top topics don't change very frequently on a daily basis, we can represent all the text in the dataset as a collection of 20 topics. Feel free to experiment with changing the number of topics to extract by modifying the NUM_TOPICS parameter.\n",
    "\n",
    "Each topic can be viewed as distinct from every other topic, allowing us to then input it as a separate timeseries into DeepAR for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IAM Roles:- In order to train a model on SageMaker, SageMaker needs to assume an IAM role to be able to access data in S3.\n",
    "## If you are running in a SageMaker notebook environment, a role has already been created when you provisioned the notebook instance.\n",
    "\n",
    "# To use that role simply run the following cell. If you get an exception, create a SageMaker IAM role first using this link: https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role-sagemaker-notebook.html\n",
    "# Make sure that along with the AmazonSageMakerFullAccess Policy you also attach a policy to provide access to your specific S3 bucket\n",
    "# This could be easily done by providing AmazonS3FullAccess but this is **not** a recommended security best practice.\n",
    "# then input that role in the text widget below.\n",
    "\n",
    "try:\n",
    "    from sagemaker import get_execution_role\n",
    "    role = get_execution_role()\n",
    "except Exception as e:\n",
    "    print( \"Enter the IAM role needed for SageMaker to access training containers\" )\n",
    "    role = input(\"IAM Role Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = os.path.join(PREFIX, 'train').replace(\"\\\\\",\"/\")\n",
    "output_prefix = os.path.join(PREFIX, 'ntm-output').replace(\"\\\\\",\"/\")\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(BUCKET, train_prefix)\n",
    "output_path = 's3://{}/{}'.format(BUCKET, output_prefix)\n",
    "\n",
    "print('Training set location', s3_train_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i)).replace(\"\\\\\",\"/\")\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname).replace(\"\\\\\",\"/\")))\n",
    "split_convert_upload(vectors, bucket=BUCKET, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the latest NTM container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm', 'latest')\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "To train the model, you can use 1 or more instances (specified by train_instance_count) and choose a strategy to either Fully Replicate the data on each instance or use ShardedByS3Key which only puts certain data shards on each instance, thus speeding up the training at the cost of each instance only seeing a fraction of the data.\n",
    "\n",
    "Another way to avoid overfitting is to introduce EarlyStopping which is done here by num_patience_epochs which ensure that the training is stopped if the change in the loss is less than the specified tolerance for a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)\n",
    "ntm.set_hyperparameters(num_topics=NUM_TOPICS, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)\n",
    "s3_train = s3_input(s3_train_data, distribution='FullyReplicated') \n",
    "ntm.fit({'train': s3_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model\n",
    "\n",
    "To generate the \"feature vectors\" for the headlines, we deploy the model first, and run inferences on the entire training dataset to obtain the topic vectors.\n",
    "**IMPORTANT** Only run the next 2 cells and enter the NTM endpoint if you HAVE already deployed the NTM model, then move on to **Test the Model**.\n",
    "\n",
    "Otherwise, don't enter anything into the widget box and skip to the cell with the heading \"**New Deployment**.\"\n",
    "\n",
    "If this is your first time running this notebook, you likely have not deployed the model yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute if you PREVIOUSLY deployed the NTM model for this notebook series and skip the cell under \"New Deployment\"\n",
    "\n",
    "#Below you will need to input your endpoint name. To find this, navigate to the AWS Console and look for SageMaker.\n",
    "#Under SageMaker go to Endpoints and find the endpoint starting with \"ntm-\". Note the name and enter it in the widget below.\n",
    "\n",
    "from sagemaker.predictor import RealTimePredictor, csv_serializer, json_deserializer\n",
    "\n",
    "\n",
    "endpoint_widget = util.create_text_widget( \"endpoint_name\", \"ONLY enter if you have already (previously) deployed the NTM model. It should start with 'ntm-\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = endpoint_widget.value\n",
    "assert endpoint, \"Endpoint not set.\"\n",
    "\n",
    "ntm_predictor = RealTimePredictor(endpoint, sagemaker_session=session, serializer=csv_serializer, deserializer=json_deserializer, content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute if you have NOT deployed the NTM model for this notebook series and you skipped the previous two cells\n",
    "#(didn't enter anything into the widget box)\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "\n",
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To do a \"sanity\" check that our topic model is working as expected, we look at the extracted topic vectors from the titles and check if the topic distribution of the title is similar to that of the corresponding headline. Remember that our model has not seen the titles before. As a measure of \"similarity\", we compute the cosine similarity for a random title and associated headline. A high cosine similarity indicates that Titles and Headlines have a similar representation in this low dimensional embedding space. \n",
    "\n",
    "A cosine similarity of the Title-Headline can also be used as a feature: well written titles that correlate well with the actual Headline may obtain a higher popularity score. This could be used to check if titles and headlines represent the content of the document accurately, but we will not explore this further in this notebook.\n",
    "\n",
    "We also visualize the Headlines in a T-SNE plot to capture the number of distinct Topic clusters that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Converted back to Dense Tensor\")\n",
    "print(\"Extracting Results ...\")\n",
    "headline_data_batch = [np.array(vectors[i:i+100].todense()) for i in range(0, vectors.shape[0], 100)]\n",
    "pred_array = []\n",
    "print(\"Data Batched and ready to go\")\n",
    "for i in range(len(headline_data_batch)):\n",
    "    results = ntm_predictor.predict(headline_data_batch[i])\n",
    "    predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "    pred_array.append(predictions)\n",
    "    sys.stdout.write(\".\")\n",
    "    sys.stdout.flush()\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array_cc = np.concatenate(pred_array, axis= 0)\n",
    "print(pred_array_cc.shape)\n",
    "print(\"Store back into dataframe\")      \n",
    "for i in range(NUM_TOPICS):\n",
    "    df['Headline_Topic_{}'.format(i)] = pred_array_cc[:, i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Dataframe back as a pre-processed csv for importing into the DeepAR Model\n",
    "df.to_csv('data/preprocessed_data.csv', index =None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data = np.array(topic_vectors.tocsr()[:10].todense())\n",
    "topic_vecs = []\n",
    "for index in range(10):\n",
    "    results = ntm_predictor.predict(topic_data[index])\n",
    "    predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "    topic_vecs.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "comparisonvec = []\n",
    "for i, idx in enumerate(range(10)):\n",
    "    comparisonvec.append([df.Headline[idx], title_column[idx], cosine_similarity(topic_vecs[i], [pred_array_cc[idx]])[0][0]])\n",
    "pd.DataFrame(comparisonvec, columns=['Headline', 'Title', 'CosineSimilarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also compare headlines with other headlines in the same Topic and different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlinecomparisonvec = []\n",
    "for i in range(10):\n",
    "    headlinecomparisonvec.append([df.Headline[10], df.Headline[i + 10], cosine_similarity([pred_array_cc[10]], [pred_array_cc[i+10]])[0][0]])\n",
    "pd.DataFrame(headlinecomparisonvec, columns=['Headline', 'Nearby Headlines', 'CosineSimilarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlinecomparisonvec = []\n",
    "for i in range(10):\n",
    "    headlinecomparisonvec.append([df.Headline[10], df.Headline[i + 50000], cosine_similarity([pred_array_cc[10]], [pred_array_cc[i+50000]])[0][0]])\n",
    "pd.DataFrame(headlinecomparisonvec, columns=['Headline', 'Far away Headlines', 'CosineSimilarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that on average the nearby topics have a higher cosine similarity than far away ones. By tweaking the vocab_size and the NUM_TOPICS parameters, you can look for a better model. \n",
    "\n",
    "For now, we choose to proceed with our current results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-SNE\n",
    "\n",
    "Another way to visualize the results is to plot a T-SNE graph. T-SNE uses a nonlinear embedding model by attempting to check if the nearest neighbor joint probability distribution in the high dimensional space (in this case NUM_TOPICS) matches the equivalent lower dimensional (in this case: 2) joint distribution by minimizing a loss known as the Kullback-Leibler divergence.\n",
    "\n",
    "Computing the T-SNE can take quite some time especially for large datasets, so we shuffle the dataset and extract only 10K Headline embeddings for the T-SNE plot.\n",
    "\n",
    "Refer to this excellent article which describes some of the advantages and pitfalls of using T-SNE in Topic Modeling. https://distill.pub/2016/misread-tsne/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarray = np.random.permutation(pred_array_cc)[:10000]\n",
    "from sklearn.manifold import TSNE\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=2000)\n",
    "tsne_results = tsne.fit_transform(smarray)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vec = [np.argmax(x) for x in smarray]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "tsnedf = pd.DataFrame()\n",
    "tsnedf['tsne-2d-one'] = tsne_results[:,0]\n",
    "tsnedf['tsne-2d-two'] = tsne_results[:,1]\n",
    "tsnedf['Topic']=topic_vec\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.lmplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue='Topic',\n",
    "    palette=sns.color_palette(\"hls\", NUM_TOPICS),\n",
    "    data=tsnedf,\n",
    "    legend=\"full\",\n",
    "    fit_reg=False\n",
    ")\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The T-SNE plot shows 4 large topics, which is consistent with the dataset containing 4 primary topics. But by expanding the dimensioanlity of the topic vectors to 20, we are allowing for the NTM model to capture more semantics between the Headlines than is captured by a single topic token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our topic modeling complete, and our data saved, we can delete the endpoint.\n",
    "ntm_predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we showed how to include semantic information from unstructured text data into time series forecasting. Metadata about items extracted from text capture high level features, but don't necessarily include any semantic information that is associated with free-form text. Often times, text data is only captured using an overall sentiment, but this misses the rich information contained in the actual text itself. Furthermore, in addition to the sentiment, the semantic content of news articles can vary over time which will no doubt affect the popularity of particular topics, how they are trending etc. This notebook shows one approach for incorporating unstructured text into time series modeling using SageMaker's built-in Neural Topic Model algorithm.\n",
    "\n",
    "There are 2 main high level steps:\n",
    "\n",
    "We first build a topic model to convert text data into topic vectors. <br/>\n",
    "We then load the corresponding topic vectors associated with our input text into the dataframe. <br/>\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
