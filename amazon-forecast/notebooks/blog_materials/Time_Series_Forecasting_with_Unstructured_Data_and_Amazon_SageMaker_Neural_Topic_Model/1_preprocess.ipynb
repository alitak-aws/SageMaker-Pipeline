{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the first of three notebooks on combining natural language processing with time series forecasting using Amazon SageMaker and Amazon Forecast. As a first step, we download the raw dataset from UCI: the dataset consists of news articles and their headlines and titles and their source on 4 major topics. Associated sentiment scores and article ratings on Facebook, GooglePlus and LinkedIn are provided.\n",
    "\n",
    "The dataset can be viewed in 2 ways:\n",
    "\n",
    "1) Regression: given an article, predict its popularity\n",
    "\n",
    "2) Given a topic, forecast the popularity of the topic on various social media channels from historical data out into the future.\n",
    "\n",
    "Since we want to leverage Amazon Forecast, we treat it as the latter problem. A major thrust of this workshop is to demonstrate how unstructured text data can be included in Forecasting problems. That will be the topic of Notebook 2 (2_NTM.ipynb) and 3 (3_Forecast.ipynb).\n",
    "\n",
    "But first, we need to download the preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00432/Data/News_Final.csv'\n",
    "r = requests.get(url, allow_redirects=True, verify=False)\n",
    "with open ('data/News_Final.csv', 'wb') as fd:\n",
    "    fd.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/News_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is primarily focused on extracting content from the headlines and title. So let's drop the source column and the IDLink relating the dataset to an internal ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['Source', 'IDLink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small sample of the dataset for visualization\n",
    "df_small = df.sample(frac = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Articles')\n",
    "plt.ylabel('Popularity')\n",
    "plt.title('Facebook News Articles')\n",
    "n, bins, patches = plt.hist(df_small['Facebook'], bins = 100, density=True, range = (0,600), alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Articles')\n",
    "plt.ylabel('Popularity')\n",
    "plt.title('GooglePlus News Articles')\n",
    "n, bins, patches = plt.hist(df_small['GooglePlus'], bins = 100, density=True, range = (0,600), alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the popularity of articles is extremely skewed. For this exercise, we may just choose to forecast the popularity on one of the platforms. In order to convert this into a usable time series for Machine Learning, we need to aggregate the news articles. We have 4 categories, let's aggregate the news datasets from all the 4 categories into 4 timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we replace the Original Topics with Numerical \"item_id\"\n",
    "df =df.replace({'Topic': {'economy':0, 'obama': 1, 'microsoft': 2, 'palestine': 3}})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we convert the PublishDate column to a datetime column using pandas to_datetime function.\n",
    "df['PublishDate'] = pd.to_datetime(df['PublishDate'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = ['Topic', 'PublishDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/NewsRatingsdataset.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End\n",
    "\n",
    "In this notebook, we downloaded the dataset and did some very basic preprocessing and cleaning as well as some simple visualizations. \n",
    "\n",
    "Next move on to the 2_NTM.ipynb notebook to preprocess the text data even further and build a neural topic model to generate topic vectors from all the Headlines. This will then become the input to a DeepAR+ forecasting algorithm in the last notebook for the Amazon Forecast service in 3_Forecast.ipynb.\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
