{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use Amazon Forecast, a fully managed service for producing time series forecasts using machine learning. \n",
    "\n",
    "We will call Forecast APIs using SageMaker for which we need to ensure that the SageMaker role associated with this Notebook environment has the AmazonForecastFullAccess policy attached to it. Please go to the IAM console and check to make sure that the role associated with the notebook has this policy attached.\n",
    "\n",
    "We also need to ensure that Forecast can access data in S3 buckets. To ensure this, in the IAM console, create a role called **Forecastbasicrole** which has AmazonS3FullAccess policy attached to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "import boto3\n",
    "import sagemaker\n",
    "# importing forecast notebook utility from notebooks/ directory\n",
    "sys.path.insert( 0, os.path.abspath(\"../../common\") )\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** This requires boto version > 1.12.39. Let's check this. If not, you will need to upgrade your boto version to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if boto3.__version__ > '1.12.39':\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError('boto3 needs to be upgraded to be later than 1.12.39. Consider running !pip install --upgrade boto3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissions and environment variables\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. You can create a new S3 bucket or use an existing S3 bucket. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Please double-check in the IAM console that you created a role called Forecastbasicrole which has AmazonS3FullAccess policy attached to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_widget_account_num = util.create_text_widget( \"ACCOUNT_NUM\", \"input your 12 digit account number\" )\n",
    "text_widget_bucket = util.create_text_widget( \"bucket\", \"input your S3 bucket name\" )\n",
    "text_widget_region_name = util.create_text_widget( \"REGION_NAME\", \"input region name.\", default_value=\"us-east-1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_NUM = text_widget_account_num.value\n",
    "assert ACCOUNT_NUM, \"ACCOUNT_NUM not set.\"\n",
    "\n",
    "REGION_NAME = text_widget_region_name.value\n",
    "assert REGION_NAME, \"REGION_NAME not set.\"\n",
    "\n",
    "bucket = text_widget_bucket.value\n",
    "assert bucket, \"bucket name not set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'web-forecast-data' #modify with your preferred prefix\n",
    "role_arn = 'arn:aws:iam::{}:role/Forecastbasicrole'.format(ACCOUNT_NUM) # Create this role in IAM. Role is needed to get permissions for Forecast to access S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the region your bucket is in is the region of the session\n",
    "session = boto3.Session(region_name=REGION_NAME)\n",
    "forecast = session.client(service_name='forecast')\n",
    "s3 = session.client('s3')\n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/preprocessed_data.csv', parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['Headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PublishDate'] = pd.to_datetime(df['PublishDate'])\n",
    "df = df.set_index('PublishDate')\n",
    "df.index = df.index.to_period('1D').to_timestamp()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data on a daily basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.DataFrame()\n",
    "topics = [0, 1, 2, 3]\n",
    "for topic in topics:\n",
    "    tdf = df[df['Topic']==topic]\n",
    "    tdf = tdf.drop(columns = ['Topic'])\n",
    "    tdf = tdf.resample('1D').mean().fillna(0)\n",
    "    itemid = np.full(len(tdf), topic)\n",
    "    tdf['Topic']=itemid\n",
    "    agg_df = pd.concat([tdf, agg_df], axis=0)\n",
    "agg_df.head()\n",
    "print(\"Shape of final Dataframe for Forecasting = {}\".format(agg_df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = agg_df.copy()\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FREQUENCY = \"D\" \n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "start_training = pd.Timestamp(\"2015-11-01\", freq = DATASET_FREQUENCY) + pd.Timedelta(days=1)\n",
    "end_training = pd.Timestamp(\"2016-06-21\", freq = DATASET_FREQUENCY)\n",
    "\n",
    "# End date for ground truth values to be used in comparison with forecasted values\n",
    "# (given we are predicting 15 days into the future, this subset will be 15 days past the end_training date)\n",
    "end_GT = pd.Timestamp(\"2016-07-05\", freq = DATASET_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Target and Related Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to forecast the Facebook ratings for each of the 4 topics in the Topic column of the dataset. In Amazon Forecast, we need to define a target time series which consists of the item id, time stamp and the value we wish to forecast. \n",
    "\n",
    "Additionally, we can provide a related time series which can include up to 13 dynamical features, which in our case are the HeadlineSentiment and the topic vectors. Since we can only choose 13 features in Amazon Forecast, we choose 10 out of the 20 topic vectors to illustrate buildng the Forecast model.\n",
    "\n",
    "As before, we start forecasting from 2015-11-01 and end our training data at 2016-06-21. Using this, we will forecast for 15 days out into the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target and Related Time series\n",
    "target_df = pd.DataFrame()\n",
    "target_df['item_id'] = df.Topic\n",
    "target_df['timestamp'] = df.index\n",
    "target_df['value'] = df.Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = target_df[(target_df['timestamp']<end_training)&(target_df['timestamp']>start_training)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Related time series only takes up to 13 features. \n",
    "related_df = pd.DataFrame()\n",
    "related_df['item_id'] = df.Topic\n",
    "related_df['timestamp'] = df.index\n",
    "related_df['SentimentHeadline'] = df.SentimentHeadline\n",
    "for i in range(10):\n",
    "    related_df['Headline_{}'.format(i)] = df['Headline_Topic_{}'.format(i)]\n",
    "related_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_df = related_df[(related_df['timestamp']>start_training)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Target and Related timeseries data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = './forecast-data'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "target_df.to_csv(os.path.join(outdir, 'target_time_series.csv').replace(\"\\\\\",\"/\"), index=False)\n",
    "related_df.to_csv(os.path.join(outdir, 'related_time_series.csv').replace(\"\\\\\",\"/\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(Filename=\"./forecast-data/target_time_series.csv\", Bucket=bucket, Key=\"{}/{}\".format(prefix, 'target_time_series.csv')\n",
    ")\n",
    "s3.upload_file(Filename=\"./forecast-data/related_time_series.csv\", Bucket=bucket, Key=\"{}/{}\".format(prefix, 'related_time_series.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset schemas to ingest into Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast has a number of predefined **Domains** which come with predefined schemas for data ingestion. Since we are interested in *web traffic*, we choose the WEB_TRAFFIC domain below.\n",
    "\n",
    "This provides a predefined schema and attribute types for the attributes we include in the target and related time series. For the WEB_TRAFFIC domain, there is no item metadata, only target and related time series data is allowed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the schema for the target time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset name to a new unique value. If it already exists, go to the Forecast console and delete any existing\n",
    "# dataset ARNs and datasets.\n",
    "\n",
    "datasetName = 'webtraffic_forecast_NLP'\n",
    "\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },    \n",
    "       {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"value\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }      \n",
    "   ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = forecast.create_dataset(\n",
    "                    Domain=\"WEB_TRAFFIC\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DatasetName=datasetName,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    Schema = schema\n",
    "                   )\n",
    "    datasetArn = response['DatasetArn']\n",
    "    print('Success')\n",
    "except forecast.exceptions.ResourceAlreadyExistsException as e:\n",
    "    print(e)\n",
    "    datasetArn = 'arn:aws:forecast:{}:{}:dataset/{}'.format(REGION_NAME, ACCOUNT_NUM, datasetName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the schema for the related time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset name to a new unique value. If it already exists, go to the Forecast console and delete any existing\n",
    "# dataset ARNs and datasets.\n",
    "\n",
    "datasetName = 'webtraffic_forecast_related_NLP'\n",
    "schema ={\n",
    "   \"Attributes\":[{\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      }, \n",
    "       {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"SentimentHeadline\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }]\n",
    "    + \n",
    "      [{\n",
    "         \"AttributeName\":\"Headline_{}\".format(x),\n",
    "         \"AttributeType\":\"float\"\n",
    "      } for x in range(10)] \n",
    "}\n",
    "\n",
    "try:\n",
    "    response=forecast.create_dataset(\n",
    "                    Domain=\"WEB_TRAFFIC\",\n",
    "                    DatasetType='RELATED_TIME_SERIES',\n",
    "                    DatasetName=datasetName,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    Schema = schema\n",
    "                   )\n",
    "    related_datasetArn = response['DatasetArn']\n",
    "    print('Success')\n",
    "except forecast.exceptions.ResourceAlreadyExistsException as e:\n",
    "    print(e)\n",
    "    related_datasetArn = 'arn:aws:forecast:{}:{}:dataset/{}'.format(REGION_NAME, ACCOUNT_NUM, datasetName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before ingesting any data into Forecast we need to combine the target and related time series into a dataset group. We define this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetGroupName = 'webtraffic_forecast_NLPgroup'\n",
    "    \n",
    "try:\n",
    "    create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=datasetGroupName,\n",
    "                                                              Domain=\"WEB_TRAFFIC\",\n",
    "                                                              DatasetArns= [datasetArn, related_datasetArn]\n",
    "                                                             )\n",
    "    datasetGroupArn = create_dataset_group_response['DatasetGroupArn']\n",
    "    print('Success')\n",
    "\n",
    "except forecast.exceptions.ResourceAlreadyExistsException as e:\n",
    "    print(e)\n",
    "    datasetGroupArn = 'arn:aws:forecast:{}:{}:dataset-group/{}'.format(REGION_NAME, ACCOUNT_NUM, datasetGroupName)\n",
    "                                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetGroupArn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=datasetGroupArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest the target and related time series data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3DataPath = 's3://{}/{}/target_time_series.csv'.format(bucket, prefix)\n",
    "datasetImportJobName = 'forecast_DSIMPORT_JOB_TARGET'\n",
    "\n",
    "try:\n",
    "    ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":s3DataPath,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          },\n",
    "                                                          TimestampFormat=TIMESTAMP_FORMAT\n",
    "                                                         )\n",
    "    ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\n",
    "    target_ds_import_job_arn = copy.copy(ds_import_job_arn) #used to delete the resource during cleanup\n",
    "except forecast.exceptions.ResourceAlreadyExistsException as e:\n",
    "    print(e)\n",
    "    ds_import_job_arn='arn:aws:forecast:{}:{}:dataset-import-job/{}/{}'.format(REGION_NAME, ACCOUNT_NUM, datasetArn, datasetImportJobName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check status -- it will change from IN PROGRESS to ACTIVE once the dataset upload is completed.\n",
    "while True:\n",
    "    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    print(dataImportStatus)\n",
    "    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3DataPath = 's3://{}/{}/related_time_series.csv'.format(bucket, prefix)\n",
    "datasetImportJobName = 'forecast_DSIMPORT_JOB_RELATED'\n",
    "try:\n",
    "    ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=related_datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":s3DataPath,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          },\n",
    "                                                          TimestampFormat=TIMESTAMP_FORMAT\n",
    "                                                         )\n",
    "    ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\n",
    "    related_ds_import_job_arn = copy.copy(ds_import_job_arn) #used to delete the resource during cleanup\n",
    "except forecast.exceptions.ResourceAlreadyExistsException as e:\n",
    "    print(e)\n",
    "    ds_import_job_arn='arn:aws:forecast:{}:{}:dataset-import-job/{}/{}'.format(REGION_NAME, ACCOUNT_NUM, related_datasetArn, datasetImportJobName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    print(dataImportStatus)\n",
    "    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While DeepAR in SageMaker is a single built-in algorithm for time series analysis, Amazon Forecast provides much greater flexibility with choosing out-of-the-box time series algorithms for model training. Additionally, there is an AutoML feature where you let Amazon Forecast choose the best model based on the data and the weighted average of the p10, p50 and p90 quantile losses.\n",
    "\n",
    "Here we choose the DeepAR+ algorithm as it is capable of building a global model based on all the different target time series data. Additonally, like the Prophet and NPTS algorithms, DeepAR+ can also incorporate information from the related time series which we provide here. \n",
    "\n",
    "For more details on the DeepAR+ algorithm and the differences between this and DeepAR, please see: https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html\n",
    "\n",
    "Currently only the DeepAR+ algorithm supports hyperparamter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorName = 'web_traffic_forecast' + 'DeepARPlus'\n",
    "forecastHorizon = 15\n",
    "algorithmArn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus' \n",
    "# choose an algorithm here or set AutoML to be true. Possible algorithmARN choices are:\n",
    "#ARIMA (no related time series): arn:aws:forecast:::algorithm/ARIMA\n",
    "#ETS (no related time series) arn:aws:forecast:::algorithm/ETS\n",
    "#NPTS: arn:aws:forecast:::algorithm/NPTS\n",
    "#Prophet: arn:aws:forecast:::algorithm/Prophet\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_predictor_response=forecast.create_predictor(PredictorName=predictorName, \n",
    "                                                  ForecastHorizon=forecastHorizon,\n",
    "                                                  AlgorithmArn=algorithmArn,\n",
    "                                                  PerformAutoML=False, # change to true if want to perform AutoML\n",
    "                                                  PerformHPO=False, # change to true to perform HPO\n",
    "                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": 1, \n",
    "                                                                         \"BackTestWindowOffset\": 15}, \n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": \"D\", \n",
    "                                                                        }\n",
    "                                                 )\n",
    "    predictorArn=create_predictor_response['PredictorArn']\n",
    "except forecast.exceptions.ResourceAlreadyExistsException as e:\n",
    "    predictorArn = 'arn:aws:forecast:{}:{}:predictor/{}'.format(REGION_NAME, ACCOUNT_NUM, predictorName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: this will take a few minutes\n",
    "while True:\n",
    "    predictorStatus = forecast.describe_predictor(PredictorArn=predictorArn)['Status']\n",
    "    print(predictorStatus)\n",
    "    if predictorStatus != 'ACTIVE' and predictorStatus != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the Predictor we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_predictor(PredictorArn=predictorArn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics from Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done creating predictor. Getting accuracy numbers for DeepAR+ ...')\n",
    "\n",
    "error_metrics_deep_ar_plus = forecast.get_accuracy_metrics(PredictorArn=predictorArn)\n",
    "error_metrics_deep_ar_plus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_metrics(metric_response, predictor_name):\n",
    "    df = pd.DataFrame(metric_response['PredictorEvaluationResults']\n",
    "                 [0]['TestWindows'][0]['Metrics']['WeightedQuantileLosses'])\n",
    "    df['Predictor'] = predictor_name\n",
    "    return df\n",
    "\n",
    "deep_ar_metrics = extract_summary_metrics(error_metrics_deep_ar_plus, \"DeepAR\")\n",
    "pd.concat([deep_ar_metrics]) \\\n",
    "    .pivot(index='Quantile', columns='Predictor', values='LossValue').plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"News_Forecast\"\n",
    "print(f\"Done fetching accuracy numbers. Creating forecaster for DeepAR+ ...\")\n",
    "forecast_name_deep_ar = f'{project}_deep_ar_plus'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response_deep_ar = forecast.create_forecast(ForecastName=forecast_name_deep_ar,\n",
    "                                                        PredictorArn=predictorArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arn_deep_ar = create_forecast_response_deep_ar['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    forecastStatus = forecast.describe_forecast(ForecastArn=forecast_arn_deep_ar)['Status']\n",
    "    print(forecastStatus)\n",
    "    if forecastStatus != 'ACTIVE' and forecastStatus != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created the forecast, let's now query the results to find out the popularity of the different topics in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(origdf, item_id, decoded_response):\n",
    "    quantile = [10, 50, 90]\n",
    "    df = pd.DataFrame()\n",
    "    origseries= origdf[origdf['Topic'] == int(item_id)].loc[start_training:end_training-dt.timedelta(days=1)].Facebook.tolist()\n",
    "    index = origdf[origdf['Topic'] == int(item_id)].loc[start_training:end_training-dt.timedelta(days=1)].index.append(pd.date_range(start = end_training-dt.timedelta(days=1), periods= forecastHorizon, freq = '1D'))\n",
    "    base_series = origseries\n",
    "    for q in quantile:\n",
    "        base_series.extend([decoded_response['p{}'.format(q)][x]['Value'] for x in range(forecastHorizon)])\n",
    "        df['Quantile_{}'.format(q)] = base_series\n",
    "        base_series = origdf[origdf['Topic'] == int(item_id)].loc[start_training:end_training-dt.timedelta(days=1)].Facebook.tolist()\n",
    "    \n",
    "    #adding ground truth to plot\n",
    "    base_series = origdf[origdf['Topic'] == int(item_id)].loc[start_training:end_GT].Facebook.tolist()\n",
    "    df['Ground Truth'] = base_series\n",
    "    \n",
    "    #reset base_series and index for next iteration\n",
    "    index = origdf[origdf['Topic'] == int(item_id)].loc[start_training:end_training-dt.timedelta(days=1)].index.append(pd.date_range(start = end_training-dt.timedelta(days=1), periods= forecastHorizon, freq = '1D'))\n",
    "    base_series = origdf[origdf['Topic'] == int(item_id)].loc[start_training:end_training-dt.timedelta(days=1)].Facebook.tolist()\n",
    "    \n",
    "    df['period']=index\n",
    "    df = df.reset_index().set_index('period')\n",
    "\n",
    "    return df, df[['Quantile_10','Quantile_50', 'Quantile_90', 'Ground Truth']][-200:].plot(figsize=(15, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item_id in range(0, 4):\n",
    "    forecast_response_deep = forecastquery.query_forecast(\n",
    "        ForecastArn=forecast_arn_deep_ar,\n",
    "        Filters={\"item_id\": str(item_id)})\n",
    "    df_forecast, plot = plot_results(df, str(item_id), forecast_response_deep['Forecast']['Predictions'])\n",
    "    if item_id == 1:\n",
    "        rmse = np.sqrt(mean_squared_error(df_forecast['Ground Truth'], df_forecast['Quantile_50']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Root Mean Square Error for the 15 day forecast is {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Things to Cleanup\n",
    "\n",
    "#### note: Deleting the Forecast takes a few minutes, but these cleanup sets must be done in order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Foreacst:\n",
    "util.wait_till_delete(lambda: forecast.delete_forecast(ForecastArn=forecast_arn_deep_ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Predictor:\n",
    "util.wait_till_delete(lambda: forecast.delete_predictor(PredictorArn=predictorArn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Import Jobs\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=target_ds_import_job_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.wait_till_delete(lambda: forecast.delete_dataset_import_job(DatasetImportJobArn=related_ds_import_job_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Datasets:\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=datasetArn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.wait_till_delete(lambda: forecast.delete_dataset(DatasetArn=related_datasetArn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the DatasetGroup: datasetGroupArn\n",
    "util.wait_till_delete(lambda: forecast.delete_dataset_group(DatasetGroupArn=datasetGroupArn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of notebooks we showed how to include unstructured text data into your Forecasting use case by leveraging both Amazon SageMaker's built-in DeepAR algorithm and Amazon Forecast which is a fully managed service.\n",
    "\n",
    "While the datasets used here are merely for illustration purposes, the content of these notebooks can be readily adapted to your particular use cases. Remember that in order to see lift from deep learning models, you almost always need a lot of data; or the models will tend to overfit and not generalize well. \n",
    "\n",
    "However most enterprises have a large amount of unstructured data available. By using Neural topic models, relevant semantic information within this unstructured text can be organized into topics, and that topic information can be leveraged as a \"feature\" input into a time series model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
